* MultitaskAIS online

- Goal: Evaluate how MultitaskAIS can be used to detect abnormal tracks in a
        stream of messages. (the model is built /a priori/.)

- Dataset: Britany dataset from 2011 to 2017.

** Runtimes

Runtimes are available through some docker images:

- ~base~ image: contains the multitaskAIS dependencies (using Conda) and the MultitaskAIS
  source code.

- ~stream~ image: contains the code of an operator detecting "abnormal"
  tracks in an AIS messages. At runtime this assumes that AIS message are
  ingested through Kafka and available on a topic keyed by mmsi.

- ~tracks~ image: contains the code that build tracks from a set of ais messages.
  This works offline with an already known set of messages. Those tracks can be
  used as input to benchmark the detection function.

- ~alertstats~ image: contains the code that benchmark the ~alert~ function 
  (/a contrario/ anomaly detection).
  
*** Build

To build the images you need to set the ~NAMESPACE~ and ~TARGET_REF~
environment variables. You need to use the MultitaskAIS root directory as the
context directory of the build.


- Get the code:
#+BEGIN_SRC bash
# one time
git clone https://github.com/msimonin/MultitaskAIS.git -b online_detection
cd MultitaskAIS
#+END_SRC

- Setup docker on g5k
#+BEGIN_SRC bash
# one time on g5k
/grid5000/code/bin/g5k-setup-docker
# get more space for the docker images
sudo-g5k sed -E -i "s/ExecStart=\/usr\/bin\/dockerd (.*)/ExecStart=\/usr\/bin\/dockerd --data-root=\/tmp\/docker \\1/" /lib/systemd/system/docker.service
s
sudo-g5k systemctl daemon-reload
sudo-g5k service docker restart
which docker
ls /srv/storage/sesame@storage1.rennes
#+END_SRC

- Build the images
#+BEGIN_SRC bash
# Image build example
export NAMESPACE=sesame
export TARGET_REF=1.0.0
# base
docker build -f docker/base/Dockerfile . -t $NAMESPACE/multitaskais_base:$TARGET_REF
# stream
docker build --build-arg NAMESPACE=$NAMESPACE --build-arg TARGET_REF=$TARGET_REF -f docker/stream/Dockerfile . -t $NAMESPACE/multitaskais_stream:$TARGET_REF
# tracks
docker build --build-arg NAMESPACE=$NAMESPACE --build-arg TARGET_REF=$TARGET_REF -f docker/tracks/Dockerfile . -t $NAMESPACE/multitaskais_tracks:$TARGET_REF
#+END_SRC

** Evaluation

*** Build all the tracks

For each year this creates all the tracks on a dedicated nodes.
Depending on the year it took approx 32 CPU x 6 hours.

#+BEGIN_SRC python :tangle build_tracks.py
from enoslib.api import play_on
from enoslib.infra.enos_g5k.provider import G5k
from enoslib.infra.enos_g5k.g5k_api_utils import get_cluster_site
from enoslib.infra.enos_g5k.configuration import Configuration, NetworkConfiguration
from enoslib.service import Docker

import logging
import os

logging.basicConfig(level=logging.INFO)


CLUSTER = "paravance"
SITE = get_cluster_site(CLUSTER)

YEARS = [f"201{i}" for i in range(1, 9)]

# claim the resources
network = NetworkConfiguration(
    id="n1",
    type="prod",
    roles=["my_network"],
    site=SITE
)
conf = (
    Configuration
    .from_settings(job_name="build_tracks",
                   job_type="allow_classic_ssh",
                   walltime="11:00:00")
    .add_network_conf(network)
    .add_machine(
        roles=["control"],
        cluster=CLUSTER,
        nodes=len(YEARS),
        primary_network=network
    )
    .finalize()
)

provider = G5k(conf)
roles, _ = provider.init()
print(roles)

d = Docker(agent=roles["control"], bind_var_docker="/tmp/docker")
d.deploy()

with play_on(roles=roles) as p:
    p.shell("ls /srv/storage/sesame@storage1.rennes")

for host, year in zip(roles["control"], YEARS):
    # inject host level vars
    host.extra.update(year=year)

with play_on(roles=roles) as p:
    # copy locally the source dir
    p.shell("rsync -avz /srv/storage/sesame@storage1.rennes/sesame/ais_britany/raw/{{ year }} /tmp/")
    # build tracks for the given year
    p.docker_container(
        name="multitaskais_tracks",
        image="registry.gitlab.inria.fr/sesame/platform/multitaskais_tracks:1.0.14",
        state="started",
        volumes=[
            "/tmp/{{ year }}:/data",
            "/tmp/result/{{ year }}:/tmp/trajectories"
        ],
        command="--master 'local[{{ ansible_processor_vcpus }}]' main.py /data/*/*/*.cdv brittany",
        # Expose the spark UI
        ports = ["4040:4040"]
    )
#+END_SRC

*** Save the tracks


#+BEGIN_SRC python :tangle save_tracks.py
# TODO merge with build_tracks
from enoslib.api import play_on, __default_python3__, __docker__
from enoslib.infra.enos_g5k.provider import G5k
from enoslib.infra.enos_g5k.g5k_api_utils import get_cluster_site
from enoslib.infra.enos_g5k.configuration import Configuration, NetworkConfiguration

import logging
import os

logging.basicConfig(level=logging.INFO)

CLUSTER = "paravance"
SITE = get_cluster_site(CLUSTER)

YEARS = [f"201{i}" for i in range(1, 9)]

# claim the resources
network = NetworkConfiguration(
    id="n1",
    type="prod",
    roles=["my_network"],
    site=SITE
)
conf = (
    Configuration
    .from_settings(job_name="build_tracks",
                   job_type="allow_classic_ssh",
                   walltime="11:00:00")
    .add_network_conf(network)
    .add_machine(
        roles=["control"],
        cluster=CLUSTER,
        nodes=len(YEARS),
        primary_network=network
    )
    .finalize()
)

provider = G5k(conf)
roles, _ = provider.init()
print(roles)

for host, year in zip(roles["control"], YEARS):
    # inject host level vars
    host.extra.update(year=year)

with play_on(roles=roles) as p:
    # copy locally the source dir
    target = "/srv/storage/sesame@storage1.rennes/sesame/generated/multitaskais/tracks"
    p.shell(f"mkdir -p {target}")
    p.shell("rsync -avz /tmp/result/{{ year }} %s" % target)
#+END_SRC

*** Benchmark the alert function

     - OAR script to launch an instance of the benchmark

      #+BEGIN_SRC bash :tangle alertstats.oar
      #!/bin/bash
      #OAR -n alertstats
      #OAR -q production
      #OAR -l nodes=1,walltime=9:00:0

      echo $1
      echo "-------------------"

      cat $1

      /grid5000/code/bin/g5k-setup-docker
      SOURCE=/srv/storage/sesame@storage1.rennes
      RESULT_DIR=/tmp/result

      # touch this
      ls $SOURCE

      docker run -e OUTPUT_FILE=$1.csv -v $SOURCE:$SOURCE -v $RESULT_DIR:/tmp/result registry.gitlab.inria.fr/sesame/platform/multitaskais_alertstats:1.0.14 $(cat $1)

      mkdir -p result
      ls /tmp
      ls $RESULT_DIR
      rsync -avz $RESULT_DIR/* result/.

      echo "-------------------"
      echo $1
      echo "-------------------"
      #+END_SRC

     - Launch the above after splitting the tracks between different jobs

       #+BEGIN_SRC bash :tangle launch_bench.sh
       # Create the index file
       find /srv/storage/sesame@storage1.rennes/sesame/generated/multitaskais/tracks/ -type f > index
       # Split into different inputs
       split -l 10000  /srv/storage/sesame@storage1.rennes/sesame/generated/multitaskais/tracks/index
       ls x* > params
       # Launch it
       oarsub --array-param-file ./params  -S ./alertstats.oar
       #+END_SRC

*** Analyse the output
**** Quantitave analyse
      #+BEGIN_SRC python :results raw :session plop
import glob

import matplotlib.pyplot as plt
import pandas as pd
from tabulate import tabulate


# Assuming everything is under the result dir...
ls = glob.iglob("result/*.csv")
df = pd.read_csv(next(ls))
for f in ls:
    df = pd.concat([df, pd.read_csv(f)])

# Number of tracks
table = [
    ["Number of tracks", len(df)],
    ["Number of abnormal tracks", len(df[df.normality == 'abnormal'])],
    ["Faulty tracks", len(df[df.status == 1])]
]
tabulate(table, headers=["", "count"], tablefmt="orgtbl")
      #+END_SRC

      #+RESULTS:
      |                           |  count |
      |---------------------------+--------|
      | Number of tracks          | 227863 |
      | Number of abnormal tracks |   5680 |
      | Faulty tracks             | 140820 |


      #+BEGIN_SRC python :results raw :session plop
# let's account only for non faulty tracks
# those which aren't been filtered out by processAIS
df_ok = df[df.status == 0] # NoneType err
tabulate(df_ok.loc[:, ["duration", "length"]].describe(), headers="keys", tablefmt="orgtbl")
      #+END_SRC

      #+RESULTS:
      |       | duration |  length |
      |-------+----------+---------|
      | count |    87043 |   87043 |
      | mean  |  2.08575 | 766.335 |
      | std   | 0.380716 | 2933.63 |
      | min   |  1.49174 |      20 |
      | 25%   |  1.80611 |     177 |
      | 50%   |  2.00386 |     359 |
      | 75%   |  2.24309 |     693 |
      | max   |  4.44137 |  184408 |

Reading: In average the alert function was able to handle approx one track every
2.08575s . In other words, a single instance of the ~stream~ operator should be
able to handle 0.5 track per second. Note that these results are CPU freq
dependent. The observed variations weren't significant between different CPU
velocity.



**** TODO Qualitative analyse
     
     Note: only one model used.

***** Break down per month of normal/abnormal tracks.

          #+BEGIN_SRC python :results raw :session plop
    # Adding some columns (year and month)
    import datetime
    df["year_start"] = df.start.map(lambda x: datetime.datetime.fromtimestamp(x).year)
    df["month_start"] = df.start.map(lambda x: datetime.datetime.fromtimestamp(x).month)
    df_count_by_month = df[df.status == 0 ].groupby(["year_start","month_start", "normality"])["track"].count()
    tabulate(df_count_by_month.unstack(), headers="keys", tablefmt="orgtbl")
          #+END_SRC
      
          #+RESULTS:
          |            | abnormal | normal |
          |------------+----------+--------|
          | (2011, 7)  |       53 |   1280 |
          | (2011, 8)  |       37 |   1301 |
          | (2011, 9)  |       25 |   1091 |
          | (2011, 10) |      100 |    992 |
          | (2011, 11) |        6 |    639 |
          | (2011, 12) |       27 |    500 |
          | (2012, 1)  |       21 |    608 |
          | (2012, 2)  |       13 |    504 |
          | (2012, 3)  |       29 |    847 |
          | (2012, 4)  |        5 |    375 |
          | (2012, 5)  |       22 |    494 |
          | (2012, 6)  |        9 |    314 |
          | (2012, 7)  |       25 |    293 |
          | (2012, 8)  |       20 |    300 |
          | (2012, 9)  |       10 |    400 |
          | (2012, 10) |        5 |    182 |
          | (2012, 11) |        1 |    148 |
          | (2012, 12) |       18 |    202 |
          | (2013, 1)  |       41 |    992 |
          | (2013, 2)  |       16 |    581 |
          | (2013, 3)  |       10 |    600 |
          | (2013, 4)  |       17 |    514 |
          | (2013, 5)  |       98 |   1361 |
          | (2013, 6)  |       29 |    725 |
          | (2013, 7)  |       74 |    928 |
          | (2013, 8)  |       33 |    591 |
          | (2013, 9)  |       49 |    565 |
          | (2013, 10) |       22 |    462 |
          | (2013, 11) |        7 |    318 |
          | (2013, 12) |       29 |    387 |
          | (2014, 1)  |       26 |    910 |
          | (2014, 2)  |       40 |    671 |
          | (2014, 3)  |       33 |   1205 |
          | (2014, 4)  |       38 |   1348 |
          | (2014, 5)  |      104 |   2267 |
          | (2014, 6)  |      119 |   1673 |
          | (2014, 7)  |       82 |    994 |
          | (2014, 8)  |       59 |   1171 |
          | (2014, 9)  |      171 |   1611 |
          | (2014, 10) |       36 |   1471 |
          | (2014, 11) |       83 |   1068 |
          | (2014, 12) |       60 |   1172 |
          | (2015, 1)  |       59 |   1635 |
          | (2015, 2)  |       51 |    971 |
          | (2015, 3)  |      129 |   1515 |
          | (2015, 4)  |       23 |    908 |
          | (2015, 5)  |      146 |   1371 |
          | (2015, 6)  |       68 |   1184 |
          | (2015, 7)  |       61 |    709 |
          | (2015, 8)  |       61 |    549 |
          | (2015, 9)  |       52 |    864 |
          | (2015, 10) |       32 |    573 |
          | (2015, 11) |       60 |    749 |
          | (2015, 12) |       41 |    757 |
          | (2016, 1)  |       53 |   1191 |
          | (2016, 2)  |       29 |    807 |
          | (2016, 3)  |       39 |    951 |
          | (2016, 4)  |       11 |    306 |
          | (2016, 5)  |      106 |   1394 |
          | (2016, 6)  |      195 |   1563 |
          | (2016, 7)  |      279 |   1292 |
          | (2016, 8)  |      211 |   1216 |
          | (2016, 9)  |      190 |   1373 |
          | (2016, 10) |      110 |   1817 |
          | (2016, 11) |       91 |   1253 |
          | (2016, 12) |       99 |   1830 |
          | (2017, 1)  |       41 |   2509 |
          | (2017, 2)  |       18 |   1479 |
          | (2017, 3)  |      132 |   2867 |
          | (2017, 4)  |       80 |   1381 |
          | (2017, 5)  |      213 |   2409 |
          | (2017, 6)  |      380 |   2092 |
          | (2017, 7)  |      186 |   1080 |
          | (2017, 8)  |      325 |   1388 |
          | (2017, 9)  |      123 |   1027 |
          | (2017, 10) |       80 |    876 |
          | (2017, 11) |       30 |    827 |
          | (2017, 12) |       86 |   1180 |
          | (2018, 1)  |       88 |   1415 |

***** Visualisation

    #+BEGIN_SRC python :tangle alertstats.py
    """
    Input:
        result/normal: directory containing the tracks deemed normal
        result/abnormal: directory containing detected abnormal tracks

    Output:
        result/figs: pdf representation of some normal tracks and abnormal tracks
        (by chunks of 10 tracks)
        Tracks on a plot belong to the same month (almost true)
    """
    import datetime
    import matplotlib.pyplot as plt
    import numpy as np
    import glob
    import pathlib

    def draw():
        """Draw all the EEZs. Visualize different simplifications."""
        # dataframe to do some stats

        def draw_some_normal():
            l = glob.glob("result/normal/*.npy")
            for ll in l[0:500]:
                track = np.load(ll)
                plt.plot(track[:,1], track[:,0], "b", alpha=0.25, linewidth=1)

        fig_dir = pathlib.Path("result/figs")
        fig_dir.mkdir(parents=True, exist_ok=True)
        draw_some_normal()

        l = glob.glob("result/abnormal/*.npy")
        l = sorted(list(l))
        n = 9
        for i, ll in enumerate(l):
            idx = i % n
            track = np.load(ll)
            track_name = pathlib.Path(ll).name
            start_time = track_name.split("-")[0]
            date = datetime.datetime.fromtimestamp(int(start_time))
            day = date.strftime("%Y-%m-%d")
            plt.plot(track[:,1],
                    track[:,0],
                    color=plt.cm.YlOrRd(float(idx)/n),
                    linewidth=1,
                    label=f"{int(track[0,n-1])}-{day}", 
                    )
            if i > 0 and i % n == 0:
                month = date.strftime("%Y-%m")
                plt.legend(loc="upper left", fontsize="xx-small")
                fig_file = pathlib.Path(fig_dir, f"{month}__{i}.pdf")
                plt.title(f"{month}__{i}")
                print(f"Plotting {month}__{i}")
                plt.savefig(fig_file)
                plt.clf()
                draw_some_normal()

    draw()

        #+END_SRC

    - One example of output out of 500:

    [[file:result/figs/2011-07__9.pdf]]


