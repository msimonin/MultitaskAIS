* MultitaskAIS online

- Goal: Evaluate how MultitaskAIS can be used to detect abnormal tracks in a
        stream of messages. (the model is built /a priori/.)

- Dataset: Britany dataset from 2011 to 2017.

** Runtimes

Runtimes are available through some docker images: 

- ~base~ image: contains the multitaskAIS dependencies (using Conda) and the MultitaskAIS 
  source code.

- ~stream~ image: contains the code of an operator detecting "abnormal" 
  tracks in an AIS messages. At runtime this assumes that AIS message are
  ingested through Kafka and available on a topic keyed by mmsi.

- ~tracks~ image: contains the code that build tracks from a set of ais messages.
  This works offline with an already known set of messages. Those tracks can be
  used as input to benchmark the detection function.


*** Build

To build the images you need to set the ~NAMESPACE~ and ~TARGET_REF~
environment variables. You need to use the MultitaskAIS root directory as the
context directory of the build.


- Get the code:
#+BEGIN_SRC bash
# one time
git clone https://github.com/msimonin/MultitaskAIS.git -b online_detection
cd MultitaskAIS
#+END_SRC

- Setup docker on g5k
#+BEGIN_SRC bash
# one time on g5k
/grid5000/code/bin/g5k-setup-docker
# get more space for the docker images
sudo-g5k sed -E -i "s/ExecStart=\/usr\/bin\/dockerd (.*)/ExecStart=\/usr\/bin\/dockerd --data-root=\/tmp\/docker \\1/" /lib/systemd/system/docker.service
s
sudo-g5k systemctl daemon-reload
sudo-g5k service docker restart
which docker
ls /srv/storage/sesame@storage1.rennes
#+END_SRC

- Build the images 
#+BEGIN_SRC bash
# Image build example
export NAMESPACE=sesame
export TARGET_REF=1.0.0
# base
docker build -f docker/base/Dockerfile . -t $NAMESPACE/multitaskais_base:$TARGET_REF
# stream
docker build --build-arg NAMESPACE=$NAMESPACE --build-arg TARGET_REF=$TARGET_REF -f docker/stream/Dockerfile . -t $NAMESPACE/multitaskais_stream:$TARGET_REF
# tracks
docker build --build-arg NAMESPACE=$NAMESPACE --build-arg TARGET_REF=$TARGET_REF -f docker/tracks/Dockerfile . -t $NAMESPACE/multitaskais_tracks:$TARGET_REF
#+END_SRC

** Evaluation

*** Build all the tracks

For each year this creates all the tracks on a dedicated nodes. 
Depending on the year it took approx 32 CPU x 6 hours. 

#+BEGIN_SRC python :tangle build_tracks.py
from enoslib.api import play_on, __default_python3__, __docker__
from enoslib.infra.enos_g5k.provider import G5k
from enoslib.infra.enos_g5k.g5k_api_utils import get_cluster_site
from enoslib.infra.enos_g5k.configuration import Configuration, NetworkConfiguration

import logging
import os

logging.basicConfig(level=logging.INFO)


CLUSTER = "paravance"
SITE = get_cluster_site(CLUSTER)

YEARS = [f"201{i}" for i in range(1, 9)]

# claim the resources
network = NetworkConfiguration(
    id="n1",
    type="prod",
    roles=["my_network"],
    site=SITE
)
conf = (
    Configuration
    .from_settings(job_name="build_tracks",
                   job_type="allow_classic_ssh",
                   walltime="11:00:00")
    .add_network_conf(network)
    .add_machine(
        roles=["control"],
        cluster=CLUSTER,
        nodes=len(YEARS),
        primary_network=network
    )
    .finalize()
)

provider = G5k(conf)
roles, _ = provider.init()
print(roles)

with play_on(roles=roles, priors=[__default_python3__, __docker__]) as p:
    p.lineinfile(
        path="/lib/systemd/system/docker.service",
        regexp="ExecStart=/usr/bin/dockerd (.*)",
        # NOTE: Yaml requires escaping backslashes in double quotes but not in single quotes
        line="ExecStart=/usr/bin/dockerd --data-root=/tmp/docker \\1",
        backrefs="yes"
    )
    p.systemd(
        name="docker",
        state="restarted",
        daemon_reload="yes"
    )
    p.apt(name="python3-pip", state="present")
    p.pip(name="docker", state="present")
    p.shell("ls /srv/storage/sesame@storage1.rennes")

for host, year in zip(roles["control"], YEARS):
    # inject host level vars
    host.extra.update(year=year)

with play_on(roles=roles) as p:
    # copy locally the source dir
    p.shell("rsync -avz /srv/storage/sesame@storage1.rennes/sesame/ais_britany/raw/{{ year }} /tmp/")
    # build tracks for the given year
    p.docker_container(
        name="multitaskais_tracks",
        image="registry.gitlab.inria.fr/sesame/platform/multitaskais_tracks:1.0.14",
        state="started",
        volumes=[
            "/tmp/{{ year }}:/data",
            "/tmp/result/{{ year }}:/tmp/trajectories"
        ],
        command="--master 'local[{{ ansible_processor_vcpus }}]' main.py /data/*/*/*.cdv brittany",
        # Expose the spark UI
        ports = ["4040:4040"]
    )
#+END_SRC

*** Save the tracks


#+BEGIN_SRC python :tangle save_tracks.py
# TODO merge with build_tracks
from enoslib.api import play_on, __default_python3__, __docker__
from enoslib.infra.enos_g5k.provider import G5k
from enoslib.infra.enos_g5k.g5k_api_utils import get_cluster_site
from enoslib.infra.enos_g5k.configuration import Configuration, NetworkConfiguration

import logging
import os

logging.basicConfig(level=logging.INFO)

CLUSTER = "paravance"
SITE = get_cluster_site(CLUSTER)

YEARS = [f"201{i}" for i in range(1, 9)]

# claim the resources
network = NetworkConfiguration(
    id="n1",
    type="prod",
    roles=["my_network"],
    site=SITE
)
conf = (
    Configuration
    .from_settings(job_name="build_tracks",
                   job_type="allow_classic_ssh",
                   walltime="11:00:00")
    .add_network_conf(network)
    .add_machine(
        roles=["control"],
        cluster=CLUSTER,
        nodes=len(YEARS),
        primary_network=network
    )
    .finalize()
)

provider = G5k(conf)
roles, _ = provider.init()
print(roles)

for host, year in zip(roles["control"], YEARS):
    # inject host level vars
    host.extra.update(year=year)

with play_on(roles=roles) as p:
    # copy locally the source dir
    target = "/srv/storage/sesame@storage1.rennes/sesame/generated/multitaskais/tracks"
    p.shell(f"mkdir -p {target}")
    p.shell("rsync -avz /tmp/result/{{ year }} %s" % target)
#+END_SRC

*** Benchmark the alert function

     - OAR script to launch an instance of the benchmark

      #+BEGIN_SRC bash :tangle alertstats.oar
      #!/bin/bash
      #OAR -n alertstats
      #OAR -l nodes=1,walltime=9:00:0

      echo $1
      echo "-------------------"

      cat $1

      /grid5000/code/bin/g5k-setup-docker
      SOURCE=/srv/storage/sesame@storage1.rennes
      RESULT_DIR=/tmp/result

      # touch this
      ls $SOURCE

      OUTPUT_DIR=/tmp/result docker run -v $SOURCE:$SOURCE -v $RESULT_DIR:/tmp/result registry.gitlab.inria.fr/sesame/platform/multitaskais_alertstats:1.0.14 $(cat $1)

      mkdir -p result
      ls /tmp
      ls $RESULT_DIR
      cp $RESULT_DIR/stats.csv result/$1.csv

      echo "-------------------"
      echo $1
      echo "-------------------"

      #+END_SRC

    - Launch the above after splitting the tracks between different jobs

      #+BEGIN_SRC bash :tangle launch_bench.sh
      # Create the index file
      find /srv/storage/sesame@storage1.rennes/sesame/generated/multitaskais/tracks/ -type f > index
      # Split into different inputs
      split -l 10000  /srv/storage/sesame@storage1.rennes/sesame/generated/multitaskais/tracks/index
      ls x* > params
      # Launch it
      oarsub --array-param-file ./params  -S ./alertstats.oar
      #+END_SRC

*** Analyse the output
      #+BEGIN_SRC python :results raw :session plop
import glob

import matplotlib.pyplot as plt
import pandas as pd
from tabulate import tabulate


# Assuming everything is under the result dir...
ls = glob.iglob("result/*.csv")
df = pd.read_csv(next(ls))
for f in ls:
    df = pd.concat([df, pd.read_csv(f)])

# Number of tracks
table = [
    ["Number of tracks", len(df)],
    ["Number of abnormal tracks", len(df[df.normality == 'abnormal'])],
    ["Faulty tracks", len(df[df.status == 1])]
]
tabulate(table, tablefmt="orgtbl")
      #+END_SRC

      #+RESULTS:
      | Number of tracks          | 235046 |
      | Number of abnormal tracks |   5958 |
      | Faulty tracks             | 147384 |


      #+BEGIN_SRC python :results raw :session plop
# let's account only for non faulty tracks
# those which aren't been filtered out by processAIS
df_ok = df[df.status == 0] # NoneType err
tabulate(df_ok.describe(), headers="keys", tablefmt="orgtbl")
      #+END_SRC

      #+RESULTS:
      |       | duration | status |  length |
      |-------+----------+--------+---------|
      | count |    87662 |  87662 |   87662 |
      | mean  |  2.08516 |      0 |  766.06 |
      | std   | 0.689601 |      0 | 3131.06 |
      | min   |  1.47651 |      0 |      20 |
      | 25%   |  1.71951 |      0 |     167 |
      | 50%   |  1.81782 |      0 |     344 |
      | 75%   |  2.03061 |      0 |     666 |
      | max   |  5.89581 |      0 |  215545 |

Reading: In average the alert function was able to handle approx one track every
2.08516s. In other word, a single instance of the ~stream~ operator should be
able to handle 0.5 track per second. Note that these results are CPU freq
dependent. The observed variations weren't significant between different CPU
velocity.
