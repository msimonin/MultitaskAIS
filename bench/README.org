* Benchmarks
- Goal: Evaluate the ~alert~ function which test the "normality" of a track.

- Inputs:
  + All the tracks built from the Brittany dataset from 2011 to 2017.
    A runtime to build them is given in the ~docker/~ subdirectory.
  + A pre trained model

*** Benchmark the alert function

    Note that we ran this benchmark on Grid'5000. Dell PowerEdge C6220 II were
    used (~paranoia~ cluster) [20 cores with 128GiB RAM each]. We partition the
    set of tracks to parallelize the benchmark. A job takes as input the set of
    tracks to test and output all the execution time of the alert function on
    each track and some metadata (e.g length of the track, timespan of the
    track).

**** Technical details

    - outputs are stored in the ~alertstats.output~ (logs and csv
      files) subdirectory.
    - inputs is taken from the ~alertstats.input~ directory.

    The job definition is:
    #+BEGIN_SRC bash :tangle alertstats.oar
    #!/bin/bash
    #OAR -n alertstats
    #OAR -O alertstats.output/logs/OAR.%jobid%.stdout
    #OAR -E alertstats.output/logs/OAR.%jobid%.stderr
    #OAR -l core=1,walltime=12:08:00
    #OAR -p cluster='paranoia'

    ls /srv/storage/sesame@storage1.rennes/

    source ~/miniconda3/etc/profile.d/conda.sh
    conda activate SESAME_PY3CPU

    # some path are hardcoded in the multitaskais project
    # let's move to a reasonable place

    cd ..

    # run the alert function on all files and record the statistics
    export OUTPUT_DIR=./bench/alertstats.output
    export OUTPUT_FILE=$(basename $1).csv
    export PYTHONUNBUFFERED=1
    python alertstats.py $(cat $1)
    #+END_SRC

    This is initialize with the following which creates the directory structure
    and the partition of the set of tracks.

    #+BEGIN_SRC bash :tangle init.sh
    #!/usr/bin/env bash

    # Some initialisations for g5k

    mkdir -p alertstats.input

    pushd alertstats.input
    # list of all tracks
    find /srv/storage/sesame@storage1.rennes/sesame/generated/multitaskais/tracks/ -type f > index
    # let's split the list of tracks
    split -l 5000 index
    # let's generate the param files that will be passed to oar
    rm params; for i in $(ls x*); do echo $(pwd)/$i>> params; done
    popd

    # create the output directory and the logs directory
    mkdir -p alertstats.output/logs
    #+END_SRC

    The launcher is the following:
    #+BEGIN_SRC bash
    oarsub --array-param-file ./alertstats.input/params  -S ./alertstats.oar
    #+END_SRC

*** Statistics
    #+BEGIN_SRC python :results raw :session plop
import glob

import pandas as pd
from tabulate import tabulate


# Assuming everything is under the result dir...
ls = glob.iglob("alertstats.output/*.csv")
df = pd.read_csv(next(ls))
for f in ls:
    df = pd.concat([df, pd.read_csv(f)])

# Number of tracks
table = [
    ["Number of tracks", len(df)],
    ["Number of abnormal tracks", len(df[df.normality == 'abnormal'])],
    ["Faulty tracks", len(df[df.status == 1])]
]
tabulate(table, headers=["", "count"], tablefmt="orgtbl")
    #+END_SRC

    #+RESULTS:
    |                           |  count |
    |---------------------------+--------|
    | Number of tracks          | 237863 |
    | Number of abnormal tracks |   5764 |
    | Faulty tracks             | 147786 |


    #+BEGIN_SRC python :results raw :session plop
# let's account only for non faulty tracks
# those which aren't been filtered out by processAIS
df_ok = df[df.status == 0] # NoneType err
tabulate(df_ok.loc[:, ["duration", "length"]].describe(), headers="keys", tablefmt="orgtbl")
    #+END_SRC

    #+RESULTS:
    |       | duration |  length |
    |-------+----------+---------|
    | count |    90077 |   90077 |
    | mean  |  2.07906 | 751.725 |
    | std   |  0.22249 | 2930.02 |
    | min   |  1.49136 |      20 |
    | 25%   |  1.93709 |     166 |
    | 50%   |  2.04686 |     348 |
    | 75%   |   2.1962 |     677 |
    | max   |  3.74857 |  184408 |

    Reading: In average the alert function was able to handle approx one track
    every 2s . In other words, a single instance of the ~stream~ operator should
    be able to handle 0.5 track per second per CPU core.

*** How often we'll need to call the alert function
    
    The processing step implies that subsequent messages will be taken into
    account if separated by at least 10 minutes. In this situation, for a given
    track (i.e. for a given mmsi), the alert function can't be triggered more
    than once every 10 minutes. As a consequence knowing how many mesages with
    unique mmsi are received in this time windows gives us a worst case scenario
    where every mesage received triggers the alert function.

**** Technical details
    - outputs are stored in the ~count_uniq_mmsi.output~ (logs and csv
      files) subdirectory.
    - inputs is taken from the ~count_uniq_mmsi.input~ directory.

    The job definition is (this requires a spark environment):
    #+BEGIN_SRC bash :tangle count_uniq_mmsi.oar
#!/bin/bash -l
#OAR -n count_uniq_mmsi
#OAR -O count_uniq_mmsi.output/logs/OAR.%jobid%.stdout
#OAR -E count_uniq_mmsi.output/logs/OAR.%jobid%.stderr
#OAR -l nodes=1,walltime=5:00:00
#OAR -p cluster='paravance'

ls /srv/storage/sesame@storage1.rennes/

source ~/miniconda3/etc/profile.d/conda.sh
conda env list
conda activate spark
conda env export
env
python -c "import sys; print(sys.path)"


# run the alert function on all files and record the statistics
export PYTHONUNBUFFERED=1

OUTPUT_DIR=./count_uniq_mmsi.output 
spark-submit --master local[32] count_uniq_mmsi.py $1 $OUTPUT_DIR
    #+END_SRC

    
The spark function is as follows:
#+BEGIN_SRC python :tangle count_uniq_mmsi.py
from __future__ import print_function

import sys
from random import random
from operator import add

import pickle
from pyspark.sql import SparkSession

import sesamelib.ais_utils as ais_utils

from datetime import datetime

PREFIX="/srv/storage/sesame@storage1.rennes/sesame/ais_britany/raw"

def decode(e):
    """Decode the message according the the ais_type."""
    m = ais_utils.decode(e, ais_type="brittany")
    base = datetime.fromtimestamp(0) # first date ever 01/01/1970
    if m is not None:
        base = datetime.fromtimestamp(m.get("tagblock_timestamp", 0))
    # the day is our granularity
    result = datetime(year=base.year,
                      month=base.month,
                      day=base.day,
                      hour=base.hour,
                      minute=10*int(base.minute/10))
    if m is None:
        return result, None
    return result, m.get("mmsi") 

def length(d_mmsi):
    d, mmsi = d_mmsi
    return datetime.timestamp(d), len(set(mmsi))

if __name__ == "__main__":
    spark = SparkSession\
        .builder\
        .appName("count")\
        .getOrCreate()
    sc = spark.sparkContext
    year = sys.argv[1]
    output_dir = sys.argv[2]

    rdd_ais = sc.textFile(f"{PREFIX}/{year}/*/*/*.cdv")
    result = rdd_ais.map(decode).groupByKey().map(length).collect()
    with open(f"{output_dir}/{year}.ais", "wb") as f:
        pickle.dump(result, f)
    spark.stop()

#+END_SRC

    The launcher is the following (params is the list of year to consider):
    #+BEGIN_SRC bash
oarsub --array-param-file ./count_by_mmsi.input/params  -S ./count_by_mmsi.oar
    #+END_SRC

    #+RESULTS:

**** Statistics

     This builds the CDF of the number of unique mmsi received in a 10 minutes time window.
    #+BEGIN_SRC python :results output
import glob
import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


ls = glob.iglob("count_uniq_mmsi.output/*.ais")
# hold the result per year
years = []
for i in ls:
    with open(i, "rb") as f:
        years.append(sorted(pickle.load(f)))
all_sorted = np.asarray(sorted([[r[0], r[1]] for year in years for r in year if r[0] > 0]))
df  = pd.DataFrame(all_sorted[:,1], index=all_sorted[:,0])

sns.set_style("whitegrid")
kwargs = {'cumulative': True}
sns.distplot(df, hist_kws=kwargs, kde_kws=kwargs)
plt.xlabel("Number of unique mmsi (10 minutes time window)")
plt.ylabel("Percentage")
plt.savefig("count_uniq_mmsi.output/uniq_mmsi.pdf")
    #+END_SRC
